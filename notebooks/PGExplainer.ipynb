{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b><u>PGExplainer with Custom BA2MOTIF Dataset</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Defining the Class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BA2MOTIFS and Custom BA2MOTIFS\n",
    "import torch\n",
    "from torch_geometric.datasets import BA2MotifDataset, ExplainerDataset\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import is_undirected, degree, to_networkx, from_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Custom BA2MOTIFS\n",
    "from torch_geometric.datasets.graph_generator import BAGraph\n",
    "from torch_geometric.datasets.motif_generator import HouseMotif, CycleMotif\n",
    "\n",
    "#BA2MOTIFS graph from scratch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx #focuses on network analysis and graph theory; can create/visualize graphs\n",
    "import inspect #for viewing source code\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#Classifier\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_max_pool, global_mean_pool\n",
    "\n",
    "#Fine tuning\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __BA2Motif from Scratch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Common in Python to group related functions in a module not encapsulated in a class. NodeView node order is correct since dictionary order is \n",
    "by insertion but don't rely on this in an algorithm.'''\n",
    "\n",
    "def create_house_motif():\n",
    "    house = nx.Graph()\n",
    "    house.add_edges_from([ #isomorphically a house, but may not always look like house; 4 nodes in cycle with 2 connected to 5th node\n",
    "        (0, 1), (1, 2), (2, 3), (3, 0),  # square base\n",
    "        (1, 4), (2, 4)  # roof\n",
    "    ])\n",
    "    return house\n",
    "\n",
    "def create_cycle_motif(size):\n",
    "    cycle = nx.Graph()\n",
    "\n",
    "    cycle.add_nodes_from(range(size))\n",
    "    \n",
    "    for i in range(size - 1):\n",
    "        cycle.add_edge(i, i+1)\n",
    "    cycle.add_edge(size - 1, 0) #connect last to first to make cycle\n",
    "    \n",
    "    return cycle\n",
    "\n",
    "\n",
    "#Give all nodes and edges same features, otherwise it's \"cheating\" by labeling the answer; adding features here instead of randomly in class so it is more flexible\n",
    "def add_node_features(G, motif_nodes):\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['motif'] = 1\n",
    "\n",
    "def add_edge_features(G, motif_edges):\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['motif'] = 1\n",
    "\n",
    "\n",
    "def attach_motif(G, motif):\n",
    "    orig_num_nodes = G.number_of_nodes() #len(graph._node)\n",
    "    orig_nodes = list(G.nodes())\n",
    "\n",
    "    motif_node_mapping = {old: orig_num_nodes + map for map, old in enumerate(motif.nodes())} #shifts motif indices by n so it doesn't override current edges\n",
    "    mapped_motif_nodes = motif_node_mapping.values()\n",
    "    \n",
    "    #Create a copy of the motif and add it to existing graph data (still no edge connecting motif to original graph)\n",
    "    G.add_nodes_from(mapped_motif_nodes)\n",
    "\n",
    "    #Map and add edges\n",
    "    motif_edge_mapping = {(u,v): (u + orig_num_nodes, v + orig_num_nodes) for (u, v) in motif.edges()}\n",
    "    mapped_motif_edges = motif_edge_mapping.values()\n",
    "    G.add_edges_from(mapped_motif_edges) \n",
    "    \n",
    "    #Choose attachment point and connect motif\n",
    "    attachment_point = random.choice(orig_nodes) #chooses random element of the list of node id's as attachment point\n",
    "    G.add_edge(attachment_point, orig_num_nodes) #n is first node of motif\n",
    "\n",
    "    #Add features\n",
    "    add_node_features(G, mapped_motif_nodes)\n",
    "    add_edge_features(G, mapped_motif_edges)\n",
    "\n",
    "    return G\n",
    "\n",
    "def generate_ba2motif_dataset(num_graphs, n, m, cycle_size, motif_prob):\n",
    "    dataset = [] #list of graphs\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_graphs):\n",
    "        ba_graph = nx.barabasi_albert_graph(n, m)\n",
    "        \n",
    "        if random.random() < motif_prob: #random number from 0-1; works by Law of Large Numbers\n",
    "            motif = create_cycle_motif(cycle_size)\n",
    "            label = 0\n",
    "        else:\n",
    "            motif = create_house_motif()\n",
    "            label = 1  # House motif label is 1\n",
    "        \n",
    "        ba_motif_graph = attach_motif(ba_graph, motif)\n",
    "        \n",
    "        dataset.append(ba_motif_graph)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Creating the Dataset - Function Approach to be Compatible with PGExplainer Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 1000\n",
    "n = 20\n",
    "m = 1\n",
    "cycle_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Dataset: 100%|\u001b[32m██████████████████████████████████\u001b[0m| 1000/1000 [00:00<00:00, 2854.59 Graphs/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nx_graphs, labels = generate_ba2motif_dataset(num_graphs, n, m, cycle_size, motif_prob = 0.5) #creates nx graphs\n",
    "\n",
    "#Convert to Data objects\n",
    "dataset = []\n",
    "for nx_graph, label in tqdm(zip(nx_graphs, labels), total = len(labels), desc=\"Creating Dataset\", unit = \" Graphs\", colour=\"green\", ncols=100):\n",
    "    data = from_networkx(nx_graph)  # converts networkx to torch Data instance; used debugging to see name and dimensions of data object\n",
    "    \n",
    "    data.x = data.motif.unsqueeze(1).float()  # change shape from 1D tensor to 2D: [num_nodes, 1]\n",
    "    #data.x = torch.ones((data.num_nodes, 1), dtype=torch.float) #Other option if features not created when generating dataset\n",
    "    del data.motif #removes this attribute from namespace, not necessarily memory\n",
    "\n",
    "    #Most GNN don't use edge features\n",
    "    data.edge_attr = data.edge_motif.unsqueeze(1).float()\n",
    "    del data.edge_motif\n",
    "    data.y = torch.tensor([label], dtype=torch.long)\n",
    "    \n",
    "    dataset.append(data)\n",
    "\n",
    "ba2_explain = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Pickling the Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pickle saves data to disk in byte stream (binary data more compact than text data); like MNIST from scratch. No whitespaces or other transformations necessary\n",
    "to decode/deserialize the data so it is also faster.'''\n",
    "\n",
    "#with open('../data/ba2-explain.pkl', 'wb') as f:\n",
    "    #pickle.dump('ba2-explain', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/ba2-explain.pkl', 'rb') as f:\n",
    "        ba2_explain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Loading Pretrained Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, dropout_rate = 0.2, num_classes = 2, pretrained_model = False):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.nn1 = Sequential(\n",
    "            Linear(num_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            Linear(32, 16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.conv1 = GINConv(self.nn1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.nn2 = Sequential(\n",
    "            Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            Linear(16, 16),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv2 = GINConv(self.nn2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.fc1 = Linear(32, num_classes) #2 classes\n",
    "\n",
    "        #Ensure original model isn't altered during fine-tuning\n",
    "        if pretrained_model:\n",
    "            temp = torch.load(f'../models/BA2-Scratch/{pretrained_model}')\n",
    "            cloned_state_dict = copy.deepcopy(temp)\n",
    "            self.load_state_dict(cloned_state_dict)\n",
    "            \n",
    "            \n",
    "            self.freeze_layers()\n",
    "            self.fc1 = Linear(32, 2) #Common to freeze earlier layers; replace desired layer for fine tuning\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        for name, param in self.named_parameters():\n",
    "                param.requires_grad = True if 'fc1' else False # Don't freeze the final layer\n",
    "\n",
    "    def node_embedding(self, x, edge_index):\n",
    "        # Process through GIN layers\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch #extracts feature matrix from graph, edge info, batch indices\n",
    "        node_embeddings = self.node_embedding(x, edge_index) #reduces dimension of nodes and edges\n",
    "\n",
    "        x_max = global_max_pool(node_embeddings, batch)\n",
    "        x_mean = global_mean_pool(node_embeddings, batch)\n",
    "        x = torch.cat([x_max, x_mean], dim=1) #combines these two poolings into single tensor\n",
    "\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    model = GIN(num_features=1)\n",
    "    model.load_state_dict(torch.load(f'../models/BA2-Scratch/{filename}', weights_only=True))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_filename = \"GIN-aug-fine-tuned-100.pth\"\n",
    "model = load_model(aug_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ba2_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'strBatch' object has no attribute 'stores_as'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure your model is on the correct device\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba2_explain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_list, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert the entire list of Data objects to a single Batch object\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/Documents/graphs-deep-learning-explainability/myenv/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/graphs-deep-learning-explainability/myenv/lib/python3.10/site-packages/torch_geometric/data/collate.py:61\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Create empty stores:\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstores_as\u001b[49m(data_list[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     63\u001b[0m follow_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(follow_batch \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m     64\u001b[0m exclude_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude_keys \u001b[38;5;129;01mor\u001b[39;00m [])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'strBatch' object has no attribute 'stores_as'"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    for data in dataset:\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = int((pred == data.y).sum())\n",
    "    \n",
    "    total = len(dataset)\n",
    "    return correct / total\n",
    "\n",
    "# Usage\n",
    "model = model.to(device)  # Ensure your model is on the correct device\n",
    "accuracy = evaluate(model, ba2_explain, device)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1) #dim1 is for certain batch; index of max val will be predicted class (0 == cycle, 1 == house)\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "test_loader = DataLoader(ba2_explain, batch_size=32, shuffle = False)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(ba2_explain, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(ba2_explain, batch_size=32, shuffle = False)\n",
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
